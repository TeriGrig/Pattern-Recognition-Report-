% ==============================================================
%  PATTERN RECOGNITION PROJECT - SLIDE DECK
%  Who Is The Killer? — Piraeus Vice
% ==============================================================

\documentclass[11pt]{beamer}

\usetheme{Madrid}
\usecolortheme{default}

\usepackage{graphicx}
\usepackage{fontspec}
\usepackage{polyglossia}
\setmainlanguage{english}
\setotherlanguage{greek}
\setmainfont{Times New Roman}
\usepackage{booktabs}
\usepackage{amsmath,amssymb}
\usepackage{tikz}
\usetikzlibrary{positioning, arrows.meta}


% Custom colors
\definecolor{UnipiBlue}{RGB}{0,51,102}
\setbeamercolor{structure}{fg=UnipiBlue}

\title{Who Is The Killer?}
\subtitle{Piraeus Vice Pattern Recognition Project}
\author[Maria, Teresa, Fotis]{Vasilaina Maria (Π23015) \\ Grigoraskou Teresa (Π22037) \\ Liveris Fotis (Π23104)}
\institute[Unipi]{Department of Informatics \\ University of Piraeus}
\date{\today}

\begin{document}

% ==============================================================
% TITLE SLIDE
% ==============================================================
\begin{frame}
  \titlepage
\end{frame}

% ==============================================================
% OUTLINE
% ==============================================================
\begin{frame}{Outline}
  \tableofcontents
\end{frame}

% ==============================================================
\section{Introduction}
% ==============================================================

\begin{frame}{Project Overview}
  \begin{block}{Objective}
    Identify the most likely killer for each crime incident in the ``Piraeus Vice'' dataset using machine learning techniques.
  \end{block}
  
  \vspace{0.5em}
  
  \begin{columns}[T]
    \begin{column}{0.48\textwidth}
      \textbf{Supervised Methods:}
      \begin{itemize}
        \item Gaussian Bayes Classifier
        \item Linear Classifier
        \item SVM (RBF kernel)
        \item Multi-Layer Perceptron
      \end{itemize}
    \end{column}
    
    \begin{column}{0.48\textwidth}
      \textbf{Unsupervised:}
      \begin{itemize}
        \item PCA for visualization
        \item k-means clustering
      \end{itemize}
    \end{column}
  \end{columns}
\end{frame}

% ==============================================================
\section{Data Description}
% ==============================================================

\begin{frame}{Dataset Overview}
  \begin{itemize}
    \item \textbf{Total incidents:} 4,800 crime cases
    \item \textbf{Target:} 8 killers (multiclass classification)
    \item \textbf{Split:} TRAIN / VAL / TEST (predefined)
  \end{itemize}
  
  \vspace{1em}
  
  \begin{columns}[T]
    \begin{column}{0.48\textwidth}
      \textbf{Continuous Features (8):}
      \begin{itemize}
        \item hour\_float
        \item latitude, longitude
        \item victim\_age
        \item temp\_c, humidity
        \item dist\_precinct\_km
        \item pop\_density
      \end{itemize}
    \end{column}
    
    \begin{column}{0.48\textwidth}
      \textbf{Categorical Features (4):}
      \begin{itemize}
        \item weapon\_code
        \item scene\_type
        \item weather
        \item vic\_gender
      \end{itemize}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Identification process overview}
\centering
\begin{tikzpicture}[scale=0.65, every node/.style={scale=0.65, font=\small},
    box/.style={
        rectangle,
        draw,
        rounded corners,
        align=center,
        minimum width=5cm,
        minimum height=0.8cm
    },
    smallbox/.style={
        rectangle,
        draw,
        rounded corners,
        align=center,
        minimum width=2cm,
        minimum height=0.7cm
    },
    arrow/.style={-Latex, thick}
]

% Top blocks

\node[box] (data) {\textbf{Piraeus Vice Dataset} \\ 
Continuous + Categorical features \\ 
TRAIN / VAL / TEST split};

\node[box, below=0.6cm of data] (prep) {\textbf{Preprocessing} \\ 
StandardScaler + OneHotEncoder \\ 
(Fitted on TRAIN only)};

% Model row

\node[smallbox, below=0.8cm of prep, xshift=-2.8cm] (bayes) {Gaussian Bayes \\ (MLE + Bayes)};
\node[smallbox, right=0.3cm of bayes] (linear) {Linear \\ Classifier};
\node[smallbox, right=0.3cm of linear] (svm) {SVM \\ (RBF)};
\node[smallbox, right=0.3cm of svm] (mlp) {MLP \\ (64,32)};
\node[smallbox, right=0.3cm of mlp] (kmeans) {PCA (m=5) \\ + k-means};

\node[box, below=0.8cm of linear] (eval) {\textbf{Evaluation on VAL} \\ 
Accuracy + Confusion Matrix};

\node[box, below=0.6cm of eval] (test) {\textbf{Final TEST Predictions} \\ 
Predicted Killer + Probabilities};

% Arrows

\draw[arrow] (data) -- (prep);

\draw[arrow] (prep.south) -- (linear.north);
\draw[arrow] (prep.south) -- (bayes.north);
\draw[arrow] (prep.south) -- (svm.north);
\draw[arrow] (prep.south) -- (mlp.north);
\draw[arrow] (prep.south) -- (kmeans.north);

\draw[arrow] (bayes.south) -- (eval.north);
\draw[arrow] (linear.south) -- (eval.north);
\draw[arrow] (svm.south) -- (eval.north);
\draw[arrow] (mlp.south) -- (eval.north);
\draw[arrow] (kmeans.south) -- (eval.north);

\draw[arrow] (eval) -- (test);

\end{tikzpicture}
\end{frame}

% ==============================================================
\section{Exploratory Analysis}
% ==============================================================

\begin{frame}{Q1: Key Variable Distributions}
  \begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{./images/Q1_diagrams/q1_hist_hour_float.png}
    \hfill
    \includegraphics[width=0.45\textwidth]{./images/Q1_diagrams/q1_hist_victim_age.png}
  \end{figure}
  
  \begin{itemize}
    \item \textbf{hour\_float:} Peak incidents during daytime (unexpected)
    \item \textbf{victim\_age:} Bimodal — very young or very old (vulnerable groups)
  \end{itemize}
\end{frame}

\begin{frame}{Q1: Gaussian Mixture vs Single Gaussian}
  \begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{./images/Q1_diagrams/q1_hour_float_single_vs_gmm.png}
  \end{figure}
  
  \begin{alertblock}{Key Insight}
    Single Gaussian inadequate — GMM (3 components) captures multiple time-of-day patterns better.
  \end{alertblock}
\end{frame}

% ==============================================================
\section{Gaussian MLE per Killer}
% ==============================================================

\begin{frame}{Q2: Maximum Likelihood Estimation}
  \begin{block}{Approach}
    For each killer $k$, estimate mean $\mu_k$ and covariance $\Sigma_k$ of continuous features using MLE on TRAIN split.
  \end{block}
  
  \vspace{0.5em}
  
  \textbf{Assumption:} Each killer's feature vector follows a multivariate Gaussian distribution:
  \[
  p(\mathbf{x} | k) = \mathcal{N}(\mathbf{x}; \mu_k, \Sigma_k)
  \]
\end{frame}

\begin{frame}{Q2: Covariance Heatmaps \& Ellipses}
  \begin{figure}
    \centering
    \includegraphics[width=0.42\textwidth]{./images/Q2_diagrams/Q2_diagrams_covariance/q2_covariance_k3.png}
    \hfill
    \includegraphics[width=0.42\textwidth]{./images/Q2_diagrams/Q2_diagrams_ellipse/q2_ellipse_hour_float_longitude_k3.png}
  \end{figure}
  
  \begin{itemize}
    \item Each killer exhibits distinct spatial-temporal patterns
    \item 2D ellipses show hour\_float vs longitude distributions
  \end{itemize}
\end{frame}

% ==============================================================
\section{Gaussian Bayes Classifier}
% ==============================================================

\begin{frame}{Q3: Multiclass Gaussian Bayes Classifier}
  \begin{block}{Method}
    Use MLE parameters from Q2 to build a Bayesian classifier:
    \[
    \hat{k} = \arg\max_k \, p(k | \mathbf{x}) \propto p(\mathbf{x} | k) \cdot p(k)
    \]
  \end{block}
  
  \vspace{1em}
  
  \begin{columns}[T]
    \begin{column}{0.48\textwidth}
      \textbf{Results:}
      \begin{itemize}
        \item TRAIN Acc: \textbf{89\%}
        \item VAL Acc: \textbf{90\%}
      \end{itemize}
    \end{column}
    
    \begin{column}{0.48\textwidth}
      \textbf{Observations:}
      \begin{itemize}
        \item Confusion matrix mostly diagonal
        \item Killers 3, 6, 7 dominate
      \end{itemize}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Q3: Decision Regions (PCA Projection)}
  \begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{./images/Q3_diagrams/q3_PCA_with_consistent_legend.png}
  \end{figure}
  
  Curved decision boundaries reflect Gaussian assumption.
\end{frame}

% ==============================================================
\section{Linear Classifier}
% ==============================================================

\begin{frame}{Q4: Linear Classifier}
  \begin{block}{Approach}
    Discriminative multiclass classification using all 12 features (8 continuous + 4 categorical via one-hot encoding).
  \end{block}
  
  \vspace{1em}
  
  \begin{columns}[T]
    \begin{column}{0.48\textwidth}
      \textbf{Results:}
      \begin{itemize}
        \item TRAIN Acc: \textbf{77\%}
        \item VAL Acc: \textbf{78\%}
      \end{itemize}
    \end{column}
    
    \begin{column}{0.48\textwidth}
      \textbf{Analysis:}
      \begin{itemize}
        \item Lower than Bayes (90\%)
        \item Linear boundaries too restrictive
      \end{itemize}
    \end{column}
  \end{columns}
  
  \vspace{0.5em}
  
  \begin{alertblock}{Limitation}
    Cannot capture nonlinear killer patterns in feature space.
  \end{alertblock}
\end{frame}

\begin{frame}{Q4: Linear Decision Boundaries}
  \begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{./images/Q4_diagrams/q4_overlay_on_q3_pca.png}
  \end{figure}
  
  Linear boundaries (straight lines) fail to separate complex killer profiles.
\end{frame}

% ==============================================================
\section{Support Vector Machine}
% ==============================================================

\begin{frame}{Q5: SVM with RBF Kernel}
  \begin{block}{Configuration}
    \begin{itemize}
      \item \textbf{Kernel:} RBF (allows nonlinear boundaries)
      \item \textbf{Strategy:} One-vs-Rest (8 binary classifiers)
      \item \textbf{Tuning:} Grid search over $C \in \{0.3, 1, 3\}$ and $\gamma \in \{\text{scale}, 0.1, 0.03\}$
    \end{itemize}
  \end{block}
  
  \vspace{1em}
  
  \begin{exampleblock}{Performance}
    \textbf{VAL Accuracy: 94\%} — Best performance so far!
  \end{exampleblock}
\end{frame}

\begin{frame}{Q5: SVM Decision Regions \& Support Vectors}
  \begin{figure}
    \centering
    \includegraphics[width=0.55\textwidth]{./images/Q5_diagrams/q5_decision_regions_pca2_rbf.png}
  \end{figure}
  
  Nonlinear boundaries effectively capture killer-specific patterns.
\end{frame}

% ==============================================================
\section{Multi-Layer Perceptron}
% ==============================================================

\begin{frame}{Q6: Neural Network (MLP)}
  \begin{block}{Architecture}
    \begin{itemize}
      \item \textbf{Hidden layers:} 2 layers (64, 32 neurons)
      \item \textbf{Activation:} ReLU
      \item \textbf{Optimizer:} Adam
      \item \textbf{Regularization:} Early stopping
    \end{itemize}
  \end{block}
  
  \vspace{1em}
  
  \begin{exampleblock}{Performance}
    \textbf{VAL Accuracy: 94\%} — Matches SVM performance!
  \end{exampleblock}
\end{frame}

\begin{frame}{Q6: Feature Importance (Permutation)}
  \begin{columns}[T]
    \begin{column}{0.55\textwidth}
      \includegraphics[width=\textwidth]{./images/Q6_diagrams/q6_top5_features.png}
    \end{column}
    
    \begin{column}{0.42\textwidth}
      \textbf{Top 5 Features:}
      \begin{enumerate}
        \item dist\_precinct\_km \\ (ΔA = 0.07)
        \item latitude (ΔA = 0.06)
        \item victim\_age (ΔA = 0.05)
        \item humidity (ΔA = 0.03)
        \item pop\_density (ΔA = 0.02)
      \end{enumerate}
    \end{column}
  \end{columns}
\end{frame}

% ==============================================================
\section{Principal Component Analysis}
% ==============================================================

\begin{frame}{Q7: PCA for Dimensionality Reduction}
  \begin{block}{Purpose}
    \begin{itemize}
      \item Visualize high-dimensional data in 2D
      \item Reduce noise and computational cost
      \item Preserve maximum variance
    \end{itemize}
  \end{block}
  
  \vspace{0.5em}
  
  \begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{./images/Q7_diagrams/q7_scree_plot.png}
  \end{figure}
  
  First few components capture most variance.
\end{frame}

\begin{frame}{Q7: PC1--PC2 Scatter (Colored by Killer)}
  \begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{./images/Q7_diagrams/q7_pca2_svm_colours.png}
  \end{figure}
  
  Some separation visible, but significant overlap remains.
\end{frame}

% ==============================================================
\section{k-means Clustering}
% ==============================================================

\begin{frame}{Q8: Unsupervised k-means in PCA Space}
  \begin{block}{Approach}
    \begin{itemize}
      \item Apply PCA (2 components) on TRAIN, VAL and TEST
      \item Run k-means with $k=8$ (number of killers)
      \item Map clusters to killers via majority voting
      \item Evaluate on VAL
    \end{itemize}
  \end{block}
  
  \vspace{1em}
  
  \begin{block}{Result}
    \textbf{81.3\%} VAL accuracy.
    Natural clusters in feature space do not perfectly align with killer identities.
  \end{block}
\end{frame}

\begin{frame}{Q8: Interpretation}
  \begin{itemize}
    \item k-means finds geometric clusters, not class labels
    \item Supervised methods leverage labeled data more effectively
    \item Useful for exploratory analysis, not final prediction
  \end{itemize}
  
  \vspace{1em}
  
  \begin{exampleblock}{Key Takeaway}
    When labels are available, supervised learning outperforms unsupervised clustering for classification tasks.
  \end{exampleblock}
\end{frame}

% ==============================================================
\section{Results Comparison}
% ==============================================================

\begin{frame}{Performance Comparison}
  \begin{table}
    \centering
    \caption{VAL Accuracy across all models}
    \begin{tabular}{lc}
      \toprule
      \textbf{Model} & \textbf{VAL Accuracy} \\
      \midrule
      SVM (RBF kernel)       & \textcolor{green!60!black}{\textbf{94.8\%}} \\
      MLP (2 hidden layers)  & \textcolor{green!60!black}{\textbf{94.4\%}} \\
      Gaussian Bayes         & 90.5\% \\
      PCA + k-means          & 81.3\% \\
      Linear Classifier      & \textcolor{red!60!black}{78.2\%} \\
      \bottomrule
    \end{tabular}
  \end{table}
  
  \vspace{0.5em}
  
  \textbf{Ranking:} SVM $\approx$ MLP $>$ Bayes $>$ PCA + $k$-means $>$ Linear
\end{frame}

\begin{frame}{Visual Insights Summary}
  \begin{columns}[T]
    \begin{column}{0.48\textwidth}
      \textbf{Exploratory:}
      \begin{itemize}
        \item GMM better than single Gaussian
        \item Victims: vulnerable age groups
        \item Spatial clustering around centers
      \end{itemize}
    \end{column}
    
    \begin{column}{0.48\textwidth}
      \textbf{Decision Boundaries:}
      \begin{itemize}
        \item Gaussian: curved, smooth
        \item Linear: straight, restrictive
        \item SVM/MLP: complex, accurate
      \end{itemize}
    \end{column}
  \end{columns}
  
  \vspace{1em}
  
  \begin{block}{Most Important Features}
    Geographic (lat/long), victim info (age, gender), scene conditions (weather, type), weapon type.
  \end{block}
\end{frame}

% ==============================================================
\section{Conclusions}
% ==============================================================

\begin{frame}{Key Findings}
  \begin{enumerate}
    \item \textbf{Nonlinear models excel:} SVM and MLP achieve 94\% accuracy
    \item \textbf{Feature importance:} Geographic + victim characteristics dominate
    \item \textbf{Gaussian assumption works:} Bayes classifier at 90\% is strong baseline
    \item \textbf{Linear limits:} Only 78\% — insufficient for complex patterns
    \item \textbf{Unsupervised falls short:} k-means cannot match supervised performance, even though it has a better accuracy than linear classifier.
  \end{enumerate}
\end{frame}

\begin{frame}{Main Contributions}
  \begin{block}{What We Did}
    \begin{itemize}
      \item Comprehensive comparison of 5 methods (generative, discriminative, nonlinear, unsupervised)
      \item Proper preprocessing pipeline (standardization, one-hot encoding)
      \item No data leakage (fit on TRAIN, apply to VAL/TEST)
      \item Systematic hyperparameter tuning
      \item Rich visualizations (heatmaps, ellipses, decision boundaries, PCA)
    \end{itemize}
  \end{block}
\end{frame}

% ==============================================================
% THANK YOU
% ==============================================================
\begin{frame}[plain]
  \begin{center}
    \Huge \textbf{Thank You!}
    
    \vspace{2em}
    
    \Large Questions?
    
    \vspace{2em}
    
    \normalsize
    \textit{Who Is The Killer? — Piraeus Vice}
    
    \vspace{0.5em}
    
    Vasilaina Maria, Grigoraskou Teresa, Liveris Fotis
    
    University of Piraeus
  \end{center}
\end{frame}

\end{document}
