% ==============================================================
%  PROJECT REPORT TEMPLATE (LIGHT VERSION)
%  Who Is The Killer? — Piraeus Vice
% ==============================================================

\documentclass[11pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{fontspec}
\usepackage{polyglossia}
\setmainlanguage{english}
\setotherlanguage{greek}
\setmainfont{Times New Roman}
\newfontfamily\greekfonttt{Courier New} %Courier New
\setmonofont{Courier New} % For minted code blocks with Greek support
\usepackage{minted}
\usepackage{xcolor}
\usemintedstyle{tango}  
\usepackage{booktabs}

\usepackage{amsmath,amssymb}
\usepackage{geometry}
\geometry{margin=2.5cm}
\usepackage{enumitem}
\usepackage{mwe} % for example-image, example-image-duck, etc.

\usepackage{tikz}
\usetikzlibrary{positioning, arrows.meta}

\usepackage{dirtree}
\usepackage{tcolorbox}

\title{
  \includegraphics[width=0.25\textwidth]{./images/UNIPI.png}\\[3em]
  Who Is The Killer?\\[0.3em]
  {\large Piraeus Vice Pattern Recognition Project} \\[1em]
}

\author{
  Student 1: Βασίλαινα Μαρία (ID: Π23015) \\
  Student 2: Γρηγοράσκου Τερέζα (ID: Π22037) \\
  Student 3: Λιβέρης Φώτης (ID: Π23104) \\
  \\[1.5em]
  Department of Informatics \\
  University of Piraeus \\[1.5em]
}

\date{\today}

\begin{document}

\maketitle

\newpage

\begin{abstract}
  
In this project, we addressed the killer identification problem as a multiclass classification task. We first estimated class-specific means and covariances using
Maximum Likelihood Estimation (MLE), and used them to build a Gaussian Bayes classifier. We then implemented a linear classifier, a nonlinear SVM (RBF kernel), a two-hidden-layer 
MLP neural network, and an unsupervised PCA + k-means approach with majority-vote label mapping. All preprocessing (standardization and one-hot encoding) was fitted on the TRAIN 
split and applied to VAL and TEST to avoid data leakage. PCA was used both for visualization and as a preprocessing step for k-means. On the validation set, nonlinear models 
(SVM and MLP) achieved the best performance, outperforming the Gaussian Bayes and linear classifiers. The PCA + k-means approach achieved lower accuracy, indicating that natural 
clusters in feature space do not perfectly align with killer identities. Overall, the features that proved to be of the most importance were those related with the victim and the 
crime scene. Last but not least, modeling nonlinear decision boundaries significantly improved classification performance. \\[0.5em]
Collectively, in order to classify data and reach our final prediction we followed the following steps.
\begin{itemize}
  \item MLE (Maximum Likelihood Estimation)
  \item Gaussian Bayes classifier
  \item Linear classifier
  \item SVM (Sypport Vector Machine)
  \item MLP (Multi-Layer Preceptron)
  \item PCA (Principal Component Analysis)
  \item k-means
\end{itemize} 
\end{abstract}

\newpage

\tableofcontents

\newpage

% --------------------------------------------------------------
\section{Introduction}

In the current project we were asked to identify the most likely killer for each crime incident in the ``Piraeus Vice'' dataset. This is formulated as a multiclass
classification problem, where each of the incidents must be related to one of the 8 possible killers based on the available features. So, the task is to build models 
that can accurately predict the killer ID. \\[0.5em]
Each row in the dataset corresponds to a single crime incident and contains the following features:

\begin{itemize}
  \item The hour that the crime was commited
  \item The latitude and the lotitude, together forming the coordinates of the crime scene
  \item The victim's age
  \item The prevailing temperature at the time the crime was commited
  \item The prevailing humidity at the time the crime took place
  \item The distance between the crime scene and the nearest police precinct
  \item The population density of the area
  \item The weapon type (knife, handgun 9mm, revolver .38, shotgun, blunt object, unknown)
  \item The scene type (street, residence, business, other)
  \item The prevailing weather conditions (clear, rain, snow, fog, unknown)
  \item The victim gender (male, female)
  \item The incident ID
  \item The partition type (TRAIN, VAL, TEST)
  \item The killer ID (1 ... 8)
\end{itemize} 

\textbf{Comment:} Further analysis will follow in section 2. \\[0.5em]
The main steps of analysis we are instructed to use are:

\begin{itemize}
  \item Q1: Exploratory Data Analysis
  \item Q2: Parameter Estimation (MLE)
  \item Q3: Gaussian Bayes Classifier
  \item Q4: Linear Classifier
  \item Q5: Support Vector MAchine (SVM)
  \item Q6: Multilayer Perceptron (MLP)
  \item Q7: Principal Component Analysis (PCA)
  \item Q8: PCA + k-means
\end{itemize}

\newpage

\noindent The following picture shows a high-level overview of the killer identification process.\\[1em]

\begin{figure}[h]
\centering
\begin{tikzpicture}[
    box/.style={
        rectangle,
        draw,
        rounded corners,
        align=center,
        minimum width=7cm,
        minimum height=1cm
    },
    smallbox/.style={
        rectangle,
        draw,
        rounded corners,
        align=center,
        minimum width=3.2cm,
        minimum height=0.9cm
    },
    arrow/.style={-Latex, thick}
]

% Top blocks

\node[box] (data) {\textbf{Piraeus Vice Dataset} \\ 
Continuous + Categorical features \\ 
TRAIN / VAL / TEST split};

\node[box, below=0.8cm of data] (prep) {\textbf{Preprocessing} \\ 
StandardScaler + OneHotEncoder \\ 
(Fitted on TRAIN only)};

% Model row

\node[smallbox, below=1.2cm of prep, xshift=-3.6cm] (bayes) {Gaussian Bayes \\ (MLE + Bayes)};
\node[smallbox, right=0.5cm of bayes] (linear) {Linear \\ Classifier};
\node[smallbox, right=0.5cm of linear] (svm) {SVM \\ (RBF)};
\node[smallbox, right=0.5cm of svm] (mlp) {MLP \\ (64,32)};
\node[smallbox, right=0.5cm of mlp] (kmeans) {PCA (m=5) \\ + k-means};

\node[box, below=1.2cm of linear] (eval) {\textbf{Evaluation on VAL} \\ 
Accuracy + Confusion Matrix};

\node[box, below=0.8cm of eval] (test) {\textbf{Final TEST Predictions} \\ 
Predicted Killer + Probabilities};

% Arrows

\draw[arrow] (data) -- (prep);

\draw[arrow] (prep.south) -- (linear.north);
\draw[arrow] (prep.south) -- (bayes.north);
\draw[arrow] (prep.south) -- (svm.north);
\draw[arrow] (prep.south) -- (mlp.north);
\draw[arrow] (prep.south) -- (kmeans.north);

\draw[arrow] (bayes.south) -- (eval.north);
\draw[arrow] (linear.south) -- (eval.north);
\draw[arrow] (svm.south) -- (eval.north);
\draw[arrow] (mlp.south) -- (eval.north);
\draw[arrow] (kmeans.south) -- (eval.north);

\draw[arrow] (eval) -- (test);

\end{tikzpicture}
\caption{High-level overview of the killer identification pipeline.}
\end{figure}

\newpage

% --------------------------------------------------------------
\section{Data description}

Τhe csv file we were given consists of a total of 4.800 rows, each one representing one crime incident. As we have already mentioned, it consists of 15 columns, one for each feature.
An overview of these features and their corresponding types follows in section 2.1. The dataset contains a predefined split column that is either TRAIN, VAL or 
TEST. This means that the data is already partitioned for modeling and evaluation. As far as we can tell the dataset is not standardised or normalised. 

\subsection{Feature overview}

\begin{table}[h]
  \centering
  \caption{Feature summary.}
  \begin{tabular}{ll}
    \toprule
    Feature name       & Type \\
    \midrule
    incident\_id       & identifier \\
    hour\_float        & continuous \\
    latitude           & continuous \\
    longitude          & continuous \\
    victim\_age        & continuous \\
    temp\_c            & continuous \\
    humidity           & continuous \\
    dist\_precinct\_km & continuous \\
    pop\_density       & continuous \\
    weapon\_code       & categorical \\
    scene\_type        & categorical \\
    weather            & categorical \\
    vic\_gender        & categorical (binary) \\
    split              & categorical \\
    killer\_id         & identifier \\
    \bottomrule
  \end{tabular}
\end{table}

% --------------------------------------------------------------
\newpage

\section{Results}

\subsection*{Q1: Exploratory analysis}
\addcontentsline{toc}{subsection}{Q1: Exploratory analysis}


The images that follow are the histograms for the key continuous variables (hour, age, latitude, longitude) that we produced.

\begin{figure}[h]
    \centering
    \begin{minipage}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{./images/Q1_diagrams/q1_hist_hour_float.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{./images/Q1_diagrams/q1_hist_victim_age.png}
    \end{minipage}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{minipage}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{./images/Q1_diagrams/q1_hist_latitude.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{./images/Q1_diagrams/q1_hist_longitude.png}
    \end{minipage}
    \\ [1em]
\end{figure}

Below is the comparison of the single Gaussian fit and a mixture of Gaussians for \texttt{hour\_float}.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{./images/Q1_diagrams/q1_hour_float_single_vs_gmm.png}
\end{figure}

\newpage

Also, we provide you with the additional two-dimensional plot involving \texttt{hour\_float} and \texttt{longtitude}.
using TRAIN+VAL and without using any labels, as requested.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{./images/Q1_diagrams/q1_scatter_hour_vs_longitude.png}
  \\[1em]
\end{figure}

Our observations around the 4 key continuous variables are the following:

\begin{itemize}
  \item Firstly, about the variable \texttt{hour\_float} the incidents are at their peak during daytime, which is somewhat unexpected.
  \item As far as the variable \texttt{victim\_age} is concerned it can easily be observed that the selected victims were either of a very
  young age or of a very old. With that said, we can safely come to the conclusion that they were targeted because they belonged
  to the most vulnerable age groups.
  \item The histogram of the variable \texttt{latitude} presents some fluctuations that are at their core closer to its average value.
  \item Lastly, concerning the variable \texttt{longtitude}, by observing both the corresponding histogram as well as the 2D scatter, it is clear 
  that the incident density is also thicker closer to its average value.
\end{itemize}

Regarding the single Gaussian, we believe that it is inadequate. We can easily see that it is not symmetric. But it is not solely that. It also ignores some of the peaks.
As a result it fails to represent correctly the given data. By splitting it to the 3-component Gaussian Mixture we achieve better interpretation of the 
times of the day as well as better representation of the data as it focuses on distinct times of the day.\\ [1em]

\newpage

\subsubsection*{Explanation}
\addcontentsline{toc}{subsubsection}{Explanation}

We have decided to use the library \textbf{GaussianMixture} that proved to be very usefull.
Also, we created the following function in order to be able to create the histograms.
As we can see it requires 5 parameters, 3 of which have default values in case they don't receive any.
The parameters are:

\begin{itemize}
  \item series: a pandas series containing numerical data,
  \item title: it describes the column we are focusing on,
  \item bins: the number or range of lines that the histogram will consist of,
  \item kde: it controls whether the result will be a histogram or a smooth curve that represents the distribution of the data,
  \item xlim: it limits the range of the x-axis. \\[1em]
\end{itemize}

\begin{minted}[
    linenos,                
    breaklines=true,        
    breakanywhere=true,     
    breakautoindent=true,   
    bgcolor= gray!10,       
    fontsize=\small
]{python}
def plot_hist_with_stats(series, title, bins=30, kde=False, xlim=None):
    s = series.dropna().values
    fig, ax = plt.subplots(figsize=(8, 4.5))
    sns.histplot(s, bins=bins, stat="density", kde=kde, edgecolor="white", color="#4C78A8", ax=ax)
    if xlim:
        ax.set_xlim(xlim)
    ax.set_title(title, fontweight="bold")
    ax.set_xlabel(title)
    ax.set_ylabel("Πυκνότητα")
    mu, sigma = np.mean(s), np.std(s, ddof=1)
    ax.axvline(mu, color="crimson", linestyle="--", label=f"μ={mu:.2f}")
    ax.legend()
    plt.tight_layout()
    return fig, ax, mu, sigma
\end{minted} 

% --------------------------------------------------------------
\newpage

\subsection*{Q2: Gaussian MLE per killer}
\addcontentsline{toc}{subsection}{Q2: Gaussian MLE per killer}

Below we present you  one covariance heatmap and one 2D projection with ellipses per killer, as requested.  

\begin{itemize}
  \item Killer 1: \\ 
  \begin{figure}[h]
    \centering
    \begin{minipage}[b]{0.52\textwidth}
        \includegraphics[width=\textwidth]{./images/Q2_diagrams/Q2_diagrams_covariance/q2_covariance_k1.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{./images/Q2_diagrams/Q2_diagrams_ellipse/q2_ellipse_hour_float_longitude_k1.png}
    \end{minipage}
    \\ [1em]
  \end{figure}
  \item Killer 2: \\ 
  \begin{figure}[h]
    \centering
    \begin{minipage}[b]{0.52\textwidth}
        \includegraphics[width=\textwidth]{./images/Q2_diagrams/Q2_diagrams_covariance/q2_covariance_k2.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{./images/Q2_diagrams/Q2_diagrams_ellipse/q2_ellipse_hour_float_longitude_k2.png}
    \end{minipage}
    \\ [1em]
  \end{figure}
  \newpage
  \item Killer 3: \\ 
  \begin{figure}[h]
    \centering
    \begin{minipage}[b]{0.52\textwidth}
        \includegraphics[width=\textwidth]{./images/Q2_diagrams/Q2_diagrams_covariance/q2_covariance_k3.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{./images/Q2_diagrams/Q2_diagrams_ellipse/q2_ellipse_hour_float_longitude_k3.png}
    \end{minipage}
    \\ [1em]
  \end{figure}
  \item Killer 4: \\ 
  \begin{figure}[h]
    \centering
    \begin{minipage}[b]{0.52\textwidth}
        \includegraphics[width=\textwidth]{./images/Q2_diagrams/Q2_diagrams_covariance/q2_covariance_k4.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{./images/Q2_diagrams/Q2_diagrams_ellipse/q2_ellipse_hour_float_longitude_k4.png}
    \end{minipage}
    \\ [1em]
  \end{figure}
  \newpage
  \item Killer 5: \\ 
  \begin{figure}[h]
    \centering
    \begin{minipage}[b]{0.52\textwidth}
        \includegraphics[width=\textwidth]{./images/Q2_diagrams/Q2_diagrams_covariance/q2_covariance_k5.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{./images/Q2_diagrams/Q2_diagrams_ellipse/q2_ellipse_hour_float_longitude_k5.png}
    \end{minipage}
    \\ [1em]
  \end{figure}
  \item Killer 6: \\ 
  \begin{figure}[h]
    \centering
    \begin{minipage}[b]{0.52\textwidth}
        \includegraphics[width=\textwidth]{./images/Q2_diagrams/Q2_diagrams_covariance/q2_covariance_k6.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{./images/Q2_diagrams/Q2_diagrams_ellipse/q2_ellipse_hour_float_longitude_k6.png}
    \end{minipage}
    \\ [1em]
  \end{figure}
  \newpage
  \item Killer 7: \\ 
  \begin{figure}[h]
    \centering
    \begin{minipage}[b]{0.52\textwidth}
        \includegraphics[width=\textwidth]{./images/Q2_diagrams/Q2_diagrams_covariance/q2_covariance_k7.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{./images/Q2_diagrams/Q2_diagrams_ellipse/q2_ellipse_hour_float_longitude_k7.png}
    \end{minipage}
    \\ [1em]
  \end{figure}
  \item Killer 8: \\ 
  \begin{figure}[h]
    \centering
    \begin{minipage}[b]{0.52\textwidth}
        \includegraphics[width=\textwidth]{./images/Q2_diagrams/Q2_diagrams_covariance/q2_covariance_k8.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{./images/Q2_diagrams/Q2_diagrams_ellipse/q2_ellipse_hour_float_longitude_k8.png}
    \end{minipage}
    \\ [1em]
  \end{figure}
\end{itemize}


\subsubsection*{Explanation}
\addcontentsline{toc}{subsubsection}{Explanation}

To estimate the mean and covariance of the continuous features for each killer on TRAIN we assumed that, for each 
killer k, the continuous feature vector follows a multivariate Gaussian distribution. To estimate the parameters we 
used Maximum Likelihood Estimation (MLE), restricting ourselves only to the TRAIN split, where the killer labels are known.
For each killer k, we collected all TRAIN incidents commited by that killer. This way, each killer's profile starts to assemble.
Lastly, we visualise our results for better interpretation.

% --------------------------------------------------------------
\newpage

\subsection*{Q3: Multiclass Gaussian Bayes classifier}
\addcontentsline{toc}{subsection}{Q3: Multiclass Gaussian Bayes classifier}

We evaluated the TRAIN accuracy and the VAL accuracy and concluded that they are equal to 89\% and 90\% respectively.
Here is the requested confusion matrix on VAL. 

\begin{table}[h]
  \centering
  \caption{Confusion matrix for the Gaussian Bayes classifier on VAL.}
  \begin{tabular}{lcccccccc}
    \toprule
    True / Pred & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
    \midrule
    1           & 13 &  1 &  2 &  0 &  0 &  1 &  0 &  0\\
    2           &  1 & 59 &  1 &  0 &  0 &  1 &  0 &  0\\
    3           &  3 &  0 &474 &  0 &  5 &  0 &  7 &  2\\
    4           &  0 &  0 &  2 & 38 &  1 &  0 &  4 &  0\\
    5           &  0 &  0 & 32 &  1 & 11 &  0 &  1 &  3\\
    6           &  0 &  0 &  0 &  0 &  0 &133 &  0 &  0\\
    7           &  1 &  2 &  0 &  3 &  0 &  0 &137 &  0\\
    8           &  0 &  0 & 15 &  0 &  2 &  0 &  0 &  2\\
    \bottomrule
  \end{tabular}
\end{table}

We can see that the matrix produced is mostly diagonal and the killers with the highest presence are the 
killers \textbf{3, 6 and 7}, which will be explained due to the accuracy of Q8 in the end of the assignment.\\
Below follows the two-dimensional projection, visualising the decision regions induced by the Bayes classifier, with 
TRAIN points coloured by true killer. Each curved boundary seperates the feature regions of each killer.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{./images/Q3_diagrams/q3_PCA_with_consistent_legend.png}
  \\[1em]
\end{figure}

\newpage

\subsubsection*{Explanation}
\addcontentsline{toc}{subsubsection}{Explanation}

We implemented a multiclass Gaussian Bayes classifier using the parameters estimated in Q2. For each killer
k, we modelled the continuous feature vector as a multivariate Gaussian distribution. For each incident x we 
calculated the long-posterior probability. Lastly, we evaluated the classifier on the VAL split by calculating 
the overall accuracy and of course prosuced the corresponding confusion matrix, as shown in thee code below.

\begin{minted}[
    linenos,                
    breaklines=true,        
    breakanywhere=true,     
    breakautoindent=true,   
    bgcolor= gray!10,       
    fontsize=\small
]{python}
def predict_proba(x):
    # returns vector of posterior probabilities for one x
    log_scores = []
    for k in killers:
        lg = np.log(pi[k]) + log_gaussian(
            x,
            mu_dict[k],
            invcov[k],
            logdet[k]
        )
        log_scores.append(lg)
    log_scores = np.array(log_scores)
    # softmax
    exp_scores = np.exp(log_scores - np.max(log_scores))
    return exp_scores / exp_scores.sum()

def predict_class(x):
    pp = predict_proba(x)
    return killers[np.argmax(pp)]

# Predict on TRAIN
ytrain_pred = np.array([predict_class(x) for x in Xtrain])
train_acc = accuracy_score(ytrain, ytrain_pred)

# Predict on VAL
yval_pred = np.array([predict_class(x) for x in Xval])
val_acc = accuracy_score(yval, yval_pred)

print("TRAIN accuracy:", train_acc)
print("VAL accuracy:", val_acc)
print("VAL confusion matrix:")
print(confusion_matrix(yval, yval_pred)) 
\end{minted} 
\begin{figure}[h]
    \centering
    \includegraphics[width= 7cm]{./images/q3-terminal.png}
\end{figure}

% --------------------------------------------------------------
\newpage

\subsection*{Q4: Linear classifier}
\addcontentsline{toc}{subsection}{Q4: Linear classifier}

This time, the TRAIN accuracy and the VAL accuracy are equal to 77\% and 78\% respectively. We need to highlight that 
these numbers are lower than Q3's. That could mean that the Gaussian assumption is more accurate. Here is the confusion
matrix on VAL. 

\begin{table}[h]
  \centering
  \caption{Confusion matrix for the Gaussian Bayes classifier on VAL.}
  \begin{tabular}{lcccccccc}
    \toprule
    True / Pred & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
    \midrule
    1           &  0 &  1 &  9 &  0 &  2 &  4 &  1 &  0\\
    2           &  1 & 38 & 17 &  0 &  0 &  3 &  3 &  0\\
    3           &  0 &  4 &473 &  0 &  4 &  1 &  9 &  0\\
    4           &  1 &  0 &  4 &  0 &  0 &  0 & 40 &  0\\
    5           &  0 &  1 & 28 &  1 & 14 &  0 &  4 &  0\\
    6           &  0 &  3 & 12 &  0 &  0 &117 &  1 &  0\\
    7           &  3 &  4 & 26 &  0 &  1 &  1 &108 &  0\\
    8           &  0 &  0 & 12 &  0 &  4 &  0 &  3 &  0\\
    \bottomrule
  \end{tabular}
\end{table}

Comparing it to Q3's matrix it obviously does not resemble a diagonal matrix which may mean that 
we have strayed from our end goal. While some numbers have slightly changed(e.g. 474 -> 473), most of them 
have completely changed. Also, the last column consists entirely of zeros. However, the killers with
the highest presence remain the same. \\

Also, in the 2D PCA projection used in Q3, we overlayed the approximate linear decision boundaries and the result is the following.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.83\textwidth]{./images/Q4_diagrams/q4_overlay_on_q3_pca.png}
  \\[1em]
\end{figure}

When compared to Q3's RCA projection one can easily see that the regions are seperated by lines and clearly
determine which one corresponds to each killer. Some may correspond to more than one killers. This happens 
because the linear model is not able to fully adapt to the shape of the data the way that the Gaussian Bayes does.

\newpage

\subsubsection*{Explanation}
\addcontentsline{toc}{subsubsection}{Explanation}

We treated the killer identification as a discriminative multiclass classification problem using the full vector consisting of the 8 continuous 
features (hour\_float, latitude, longitude, victim\_age, temp\_c, humidity, dist\_precinct\_km and pop\_density) and 4 more cateforical features 
(weapon\_code, scene\_type, weather, vic\_gender). We implemented the linear classifier given. We trained the model on the TRAIN split using MSE 
(Mean Squared Error loss). Lastly, we valuate the accuracy and the confusion matrix and produce the 2D PCA overlaying the one from Q3.\\

\begin{minted}[
    linenos,                
    breaklines=true,        
    breakanywhere=true,     
    breakautoindent=true,   
    bgcolor= gray!10,       
    fontsize=\small
]{python}
Xtr_t = torch.tensor(Xfull_train, dtype=torch.float32).to(device)  # (N, d_full)
Xva_t = torch.tensor(Xfull_val,   dtype=torch.float32).to(device)
ytr_1h = torch.eye(S, dtype=torch.float32)[torch.tensor(ytrain_idx)].to(device)

d_full = Xfull_train.shape[1]
linear = nn.Linear(d_full, S).to(device)
criterion = nn.MSELoss()
opt = optim.Adam(linear.parameters(), lr=1e-3)

linear.train()
for epoch in range(1500):
    opt.zero_grad()
    out = linear(Xtr_t)             # logits (S)
    loss = criterion(out, ytr_1h)   # SSE με one-hot
    loss.backward()
    opt.step()
    if epoch % 300 == 0:
        print(f"[Q4] epoch {epoch}, loss={loss.item():.4f}")

linear.eval()
with torch.no_grad():
    tr_logits = linear(Xtr_t).cpu().numpy()
    va_logits = linear(Xva_t).cpu().numpy()
tr_pred_idx = tr_logits.argmax(1)
va_pred_idx = va_logits.argmax(1)
print("Q4 TRAIN acc:", accuracy_score(ytrain_idx, tr_pred_idx))
print("Q4 VAL acc:",   accuracy_score(yval_idx,   va_pred_idx))
print("Q4 VAL confusion:\n", confusion_matrix(yval_idx, va_pred_idx))
\end{minted}

\textbf{Note to teacher:} As you may have noticed if you have run our code, on the terminal also appear the Q3's results. Everything on terminal is exclusively 
for our convenience in order to make sure all things run smoothly and are executed correctly. It should not consern you as we have provided you with 
Q4's results in the previous section.


% --------------------------------------------------------------
\newpage

\subsection*{Q5: Support Vector Machine}
\addcontentsline{toc}{subsection}{Q5: Support Vector Machine}

The VAL accuracy for the RBF method is 94\%. The confusion matrix is the following. Once again it slightly resembles a diagonal matrix which means we are getting closer.\\

\begin{table}[h]
  \centering
  \caption{Confusion matrix for the Gaussian Bayes classifier on VAL.}
  \begin{tabular}{lcccccccc}
    \toprule
    True / Pred & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
    \midrule
    1           & 14 &  0 &  3 &  0 &  0 &  0 &  0 &  0\\
    2           &  0 & 62 &  0 &  0 &  0 &  0 &  0 &  0\\
    3           &  0 &  0 &484 &  0 &  1 &  0 &  2 &  4\\
    4           &  0 &  0 &  1 & 35 &  1 &  0 &  8 &  0\\
    5           &  0 &  0 & 14 &  0 & 30 &  0 &  1 &  3\\
    6           &  0 &  0 &  0 &  0 &  0 &133 &  0 &  0\\
    7           &  0 &  0 &  0 &  1 &  1 &  0 &141 &  0\\
    8           &  0 &  0 &  6 &  0 &  3 &  0 &  0 & 10\\
    \bottomrule
  \end{tabular}
\end{table}

In the images that follow we can see the decision regions as well as the support vectors.\\

\begin{figure}[h]
  \centering
  \includegraphics[width=0.66\textwidth]{./images/Q5_diagrams/q5_decision_regions_pca2_rbf.png}
\end{figure}

\newpage

\begin{figure}[h]
  \centering
  \includegraphics[width=0.66\textwidth]{./images/Q5_diagrams/q5_support_vectors_pca2_rbf.png}
\end{figure}

Highlighted in red are distributed the incidents that determine the characteristics of the killers' profiles.

\subsubsection*{Explanation}
\addcontentsline{toc}{subsubsection}{Explanation}

Our SVM setup is formed by choosing either RBF or Polynomial kernel. We prefered RBF as it allows non-linear decision boundaries. The code is modified for either one.\\

\begin{minted}[
    linenos,                
    breaklines=true,        
    breakanywhere=true,     
    breakautoindent=true,   
    bgcolor= gray!10,       
    fontsize=\small
]{python}
def small_search_space(kernel: str):
    if kernel == "rbf":
        Cs = [0.3, 1, 3]
        gammas = ["scale", 0.1, 0.03]
        return [(c, g, None, None) for c in Cs for g in gammas]
    else:
        Cs = [0.3, 1, 3]
        gammas = ["scale", 0.1]
        degrees = [2]
        coef0s = [0, 1]
        return [(c, g, d, c0) for c in Cs for g in gammas for d in degrees for c0 in coef0s]
\end{minted} 

We continued with tuning the hyperparameters using a small manual grin search on VAL. Each candidate model was trained on TRAIN and evaluated on VAL. We chose the 
one that seemed more suitable to us based on the highest validation accuracy. Latsly, since the SVM is a binary classifier due to inheritance, we used One vs Rest 
strategy. This way, there is one binary SVN trained per killer, which means we have 8 classifiers, each one able to seperate class k from the rest and as far as prediction is concerned, 
it is the class with the highest score that is selected. All preprocessing was applied inside a Pipeline thus ensuring there will be no data leakage.

% --------------------------------------------------------------
\newpage

\subsection*{Q6: Multi-Layer Perceptron}
\addcontentsline{toc}{subsection}{Q6: Multi-Layer Perceptron}

The VAL accuracy is equal to 94\% (same as the SVM's). \\
The features of most importance seem to be(in ascending order): \\

\begin{enumerate}

  \item dist\_previnct\_km \qquad \qquad ΔΑ = 0.07
  \item latitude \qquad \qquad \qquad \qquad ΔΑ = 0.06
  \item victim\_age \qquad \qquad \qquad \hspace{0.13cm} ΔΑ = 0.05
  \item humidity \qquad \qquad \qquad \quad \hspace{0.06cm} ΔΑ = 0.03
  \item pop\_density \qquad \qquad \quad \hspace{0.35cm} ΔΑ = 0.02 
  \\[1.5em]
\end{enumerate}

\begin{figure}[h]
  
  \includegraphics[width=0.9\textwidth]{./images/Q6_diagrams/q6_top5_features.png}
  \\[1em]
\end{figure}

\newpage

\subsubsection*{Explanation}
\addcontentsline{toc}{subsubsection}{Explanation}

We implemented a Multilayer Perceptron (MLP) as classifier using the following code:

\begin{minted}[
    linenos,                
    breaklines=true,        
    breakanywhere=true,     
    breakautoindent=true,   
    bgcolor= gray!10,       
    fontsize=\small
]{python}
mlp = MLPClassifier(
    hidden_layer_sizes=(64, 32),   # 2 hidden layers
    activation="relu",
    solver="adam",
    max_iter=200,
    random_state=0,
    early_stopping=True
)
\end{minted} 

We continued by once again training the model on TRAIN, calculated VAL accuracy etc. Lastly, feature importance was estimated using permutation importance.

\begin{minted}[
    linenos,                
    breaklines=true,        
    breakanywhere=true,     
    breakautoindent=true,   
    bgcolor= gray!10,       
    fontsize=\small
]{python}
Z_val = model.named_steps["pre"].transform(X_val)

importances = []
rng = np.random.default_rng(0)

for j in range(Z_val.shape[1]):
    Zp = Z_val.copy()
    Zp[:, j] = rng.permutation(Zp[:, j])  # permute one feature
    pred = model.named_steps["mlp"].predict(Zp)
    Aj = accuracy_score(y_val, pred)
    importances.append((all_feat[j], A_base - Aj))

# Sort by importance
importances.sort(key=lambda x: x[1], reverse=True)
top5 = importances[:5]

print("\nTop 5 Most Important Features:")
for name, imp in top5:
    print(f"{name:35s} ΔA = {imp}")
\end{minted} 

The code presented computes the permutation feature importance by measuring how much the model's accuracy decreases when each feature is randomly permuted. 
First, the validation data are transformed using the preprocessing step of the pipeline so that the MLP receives inputs in the same format as during training. 
Then, for each feature, a copy of the transformed validation set is created and the values of that feature are randomly shuffled, breaking its relationship with 
the target variable. The modified dataset is passed to the trained MLP to obtain new predictions, and the resulting accuracy is compared with the original validation 
accuracy. The drop in accuracy quantifies the importance of that feature: a larger decrease indicates that the feature is more critical for the model’s performance. 
Finally, all features are sorted by their impact, and the five most important ones are printed.

% --------------------------------------------------------------
\newpage

\subsection*{Q7: Principal Component Analysis}
\addcontentsline{toc}{subsection}{Q7: Principal Component Analysis}

Below we provide you with a plot of eigenvalues versus component index and a PC1--PC2 scatter plot coloured by predicted killer labels, as requested.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{./images/Q7_diagrams/q7_scree_plot.png}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{./images/Q7_diagrams/q7_pca2_svm_colours.png}
\end{figure}

\newpage

\subsubsection*{Explanation}
\addcontentsline{toc}{subsubsection}{Explanation}

Before applying PCA, we started off by preprocessing the features this way once again ensuring no data leakage.

\begin{minted}[
    linenos,                
    breaklines=true,        
    breakanywhere=true,     
    breakautoindent=true,   
    bgcolor= gray!10,       
    fontsize=\small
]{python}
pre = ColumnTransformer([
    ("scaler", StandardScaler(), CONT),
    ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=False), CAT)
])
\end{minted} 

Subsequently, PCA was fitted on TRAIN only and then applied to both TRAIN and VAL. Firstly, we let PCA run without any restrictions on the number of 
components. However, for visualization purposes only we chose to set the parameter n\_components to 2 to achieve projection in $R^2$.

% --------------------------------------------------------------
\newpage

\subsection*{Q8: $k$-means in PCA space}
\addcontentsline{toc}{subsection}{Q8: $k$-means in PCA space}

Below, we provide you with the plot with our final predicted killer labels Ci.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{./images/Q8_diagrams/q8_clusters_plot.png}
\end{figure}

As you can see we came to the conclusion that there are 4 predicted killers in k-means clustering. These are killers 2, 3, 6 and 7. We would like to remind 
you that in Q3 we had predicted killers 3, 6 and 7. This means that even though the VAL accuracy of k-means is equal to 81\%, it is still very close to the 
Gaussian Bayes and SVM models which have better accuracies.   

\subsubsection*{Explanation}
\addcontentsline{toc}{subsubsection}{Explanation}

To start with, the data was once again preprocessed, fitted on TRAIN and applied on VAL and TEST. The PCA was applied on TRAIN only. The number of 
components was set to 2 (based on Q7's results). So, we ran k-means with k equal to S, where equals to the nnumber of killers(in our case 8). K-means
is unsupervised, so the clusters do not correspond directly to the killer IDs. In order to convert clusters into class labels, we used majority voting 
on the TRAIN set. For each cluster, we identified all TRAIN samples assigned to that cluster, examined their true killer labels, and assigned each cluster 
the label of the most frequent killer in that cluster. For evaluation on VAL, samples were assigned to clusters using the fitted k-means model, mapped to 
killer IDs using the majority-vote mapping, and accuracy was of course calculated. Finally, for the TEST set, we produced a probability distribution over 
the clusters by calculating the Euclidean distance from each sample to all centroids. Distances were converted to scores using score = −distance, and a 
softmax approach was applied to obtain normalized probabilities.\\

\begin{minted}[
    linenos,                
    breaklines=true,        
    breakanywhere=true,     
    breakautoindent=true,   
    bgcolor= gray!10,       
    fontsize=\small
]{python}
test_clusters = kmeans.predict(Z_test_m)
test_pred = np.array([cluster_to_killer[c] for c in test_clusters])

distances = kmeans.transform(Z_test_m)

# 2. Μετατροπή αποστάσεων σε πιθανότητες (Softmax approach)
# Χρησιμοποιούμε exp(-dist) ώστε οι μικρές αποστάσεις να δίνουν μεγάλες πιθανότητες
inv_distances = np.exp(-distances)
# Κανονικοποίηση ώστε το άθροισμα κάθε γραμμής να είναι 1 [cite: 91]
probs_clusters = inv_distances / np.sum(inv_distances, axis=1, keepdims=True)

# 3. Δημιουργία DataFrame
submission = pd.DataFrame({"incident_id": test["incident_id"], "predicted_killer": test_pred})

# 4. Υπολογισμός πιθανοτήτων για κάθε killer_id (1 έως S)
for k in range(1, S + 1):
    # Βρίσκουμε ποια clusters (q) αντιστοιχούν στον δολοφόνο k [cite: 218]
    matching_clusters = [q for q, killer in cluster_to_killer.items() if killer == k]

    if matching_clusters:
        # Αθροίζουμε τις πιθανότητες των clusters που ανήκουν στον δολοφόνο k
        submission[f"p_killer_{k}"] = probs_clusters[:, matching_clusters].sum(axis=1)
    else:
        # Αν ο δολοφόνος k δεν κέρδισε κανένα cluster, του δίνουμε μια πολύ μικρή
        # βασική πιθανότητα (π.χ. από το πλησιέστερο cluster) για να μην είναι 0
        submission[f"p_killer_{k}"] = 1e-5

    # Επανεξισορρόπηση (Normalization) για να αθροίζουν ακριβώς στο 1 [cite: 91]
prob_cols = [f"p_killer_{k}" for k in range(1, S + 1)]
submission[prob_cols] = submission[prob_cols].div(submission[prob_cols].sum(axis=1), axis=0)
\end{minted} 

This code for the TEST set predicts clusters and labels similarly, then derives class probabilities by first obtaining distances of each example to all
cluster centroids (kmeans.transform). Distances are converted to soft probabilities with a softmax over negative distances (exp(-distance)), so nearer 
centroids receive higher probability mass. These cluster‑level probabilities are aggregated to class‑level probabilities by summing over all clusters 
mapped to the same killer ID. The classes with no winning cluster get a tiny fallback probability to avoid exact zeros(1e-5). Finally, the per‑row 
probabilities are renormalized to sum to 1, and a submission.csv is created containing the incident\_id, the predicted label, and the calibrated 
probabilities p\_killer\_1 … p\_killer\_S.

% --------------------------------------------------------------
\newpage

\section{Discussion and conclusions}

Let's summarise the VAL accuracies of these methods up until now:

\begin{table}[h]
  \centering
  \caption{Validation accuracy for different models.}
  \begin{tabular}{lcc}
    \toprule
    Model                  & VAL accuracy \\
    \midrule
    Gaussian Bayes         & 0.905 \\
    Linear classifier      & 0.782 \\
    SVM (RBF kernel)       & 0.948 \\
    MLP                    & 0.944 \\
    PCA + k-means          & 0.813 \\
    \bottomrule
  \end{tabular}
  \\[1em]
\end{table}

So, in ascending order of performance we have:

\begin{enumerate}
  \item SVM (Q5)
  \item MLP (Q6)
  \item Gaussian Bayes (Q3)
  \item Linear classifier (Q4)
  \item PCA + k-means (Q8)
  \\[0.2em]
\end{enumerate}

We have concluded that the more helpful features in order to model behavior were geographic features (latitude and longtitude), victim-related information 
(their age and gender), information about the place where the crime was commited and the prevailing weather conditions (weather, temperature and scene type) 
and of course the weapon type (not necessarily in that order). With the use of all presented diagrams, confusion matrixes and the final file submision.csv, 
we highlighted the 4 killers (2, 3, 6 and 7) as the killers who are most likely to have commited the most crimes, especially in TEST. Potentially, more complex generative 
models could be included and explored. Additionally, there are other factors to could be analyzed and taken into consideration, such as crime frequency patterns, killer's 
motive or mental health disorders. Overall, the current study highlights the importance of combining proper preprocessing, nonlinear modeling and feature analysis when trying 
to solve real-world pattern recognition problems.

% --------------------------------------------------------------
\newpage

\section*{Code organisation}
We implemented the code in a modular way, creating one script per question. Each script is responsible for producing the results and diagrams of its corresponding question.
The diagrams from each question are stored in a separate folder. The dataset is also included in the main folder. Lastly, the prediction file \texttt{submission.csv} is also included in the main folder.\\

\dirtree{%
.1 pattern\_recognition.
.2 Q1\_Diagrams.
.2 Q2\_Diagrams.
.2 Q3\_Diagrams.
.2 Q4\_Diagrams.
.2 Q5\_Diagrams.
.2 Q6\_Diagrams.
.2 Q7\_Diagrams.
.2 Q8\_Diagrams.
.2 crimes.csv.
.2 q1.py.
.2 q2.py.
.2 q3.py.
.2 q4.py.
.2 q5.py.
.2 q6.py.
.2 q7.py.
.2 q8.py.
.2 submission.csv.
}
\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Library imports]
\begin{minted}[breaklines=true]{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import norm
from sklearn.mixture import GaussianMixture
from numpy.linalg import inv #used in q2

#libraries used in q3, q4, q5, q6, q7, q8
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

#libraries used in q4
import torch
import torch.nn as nn
import torch.optim as optim

#libraries used in q5
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay


#libraries used in q6
from sklearn.neural_network import MLPClassifier

#libraries used in q7
from sklearn.compose import ColumnTransformer

#libraries used in q8
from sklearn.cluster import KMeans
    
\end{minted}
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Data load from q1.py]
\begin{minted}[breaklines=true,breakautoindent=true,breakanywhere=true]{python}
# Φόρτωση & φιλτράρισμα
csv_path = "crimes.csv"
df = pd.read_csv(csv_path)

# Κρατάμε μόνο TRAIN+VAL
df = df[df["split"].isin(["TRAIN", "VAL"])].copy()

# Τυπική επιβεβαίωση ότι έχουμε όλες τις απαιτούμενες στήλες
required_cols = ["hour_float", "victim_age", "latitude", "longitude"]
missing = [c for c in required_cols if c not in df.columns]
assert not missing, f"Λείπουν στήλες: {missing}"
\end{minted}
\tcblower
In this specific script we load the required libraries and the data from the \texttt{crimes.csv} file.
We split the data into TRAIN and VAL as it is required for Q1, a similar method is used for all the other questions.
Lastly, we confirm that all the required columns are present in the dataset. This is a common practice to avoid errors later on.
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Histograms Function from q1.py]
    \begin{minted}[breaklines=true]{python}
        # Συνάρτηση για ιστογράμματα
    def plot_hist_with_stats(series, title, bins=30, kde=False, xlim=None):
    s = series.dropna().values
    fig, ax = plt.subplots(figsize=(8, 4.5))
    sns.histplot(s, bins=bins, stat="density", kde=kde, edgecolor="white", color="#4C78A8", ax=ax)
    if xlim:
        ax.set_xlim(xlim)
    ax.set_title(title, fontweight="bold")
    ax.set_xlabel(title)
    ax.set_ylabel("Πυκνότητα")
    # Περιγραφικά στατιστικά
    mu, sigma = np.mean(s), np.std(s, ddof=1)
    ax.axvline(mu, color="crimson", linestyle="--", label=f"μ={mu:.2f}")
    ax.legend()
    plt.tight_layout()
    return fig, ax, mu, sigma
    \end{minted}
    \tcblower
    This function creates histograms with statistics. It takes as an input a pandas Series and drops all NaN values.
    With the command \texttt{sns.histplot} it creates the histogram. The mean value and the standard deviation are calculated with the commands
    \texttt{np.means(s)} and \texttt{np.std(s, ddof=1)} respectively. Finally, a vertical line is plotted at the mean value and the figure is returned.
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Graph Building from q7.py]
\begin{minted}[breaklines=true,breakautoindent=true,breakanywhere=true]{python}
# Q7c — Colour VAL points by SVM predictions
# (Assumes you trained model_svm in Q5)

svm_pred_val = model_svm.predict(X_val)

plt.figure(figsize=(7,6))
scatter = plt.scatter(Z_val_2D[:,0], Z_val_2D[:,1],
                      c=svm_pred_val, cmap="tab20", s=15)
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.title("Q7 – PCA Projection of VAL (coloured by SVM predictions)")
plt.colorbar(scatter, label="Predicted Killer (SVM)")
plt.grid(True)
plt.savefig("q7_pca2_svm_colours.png", dpi=150)
\end{minted}
\tcblower
In this specific code snippet we create the PCA projection of VAL coloured by SVM predictions.
Then, we create a scatter plot of the PCA projection where each point is coloured according to its predicted killer label from the SVM model. Finally, we save the plot as an image file.
\\[1em]
This code is used similarly in all the questions that require the creation of diagrams. The only difference is the data that is used for the scatter plot and the colouring.
\end{tcolorbox}

For the prediction file \texttt{submission.csv} the code is included on Q8.

% --------------------------------------------------------------
\newpage

\section{LLM prompts and responses}

The contribution of an AI, specifically Copilot, in this project was  purely auxiliary. In general, we consulted Copilot in order to provide 
us with formulas, types, libraries and assistance in diagrams' creation. In particular, we list all prompts and the 
corresponding responses we used during the preparation of this project.\\

\textbf{Comment:} As far as Q3 is concerned, we asked Copilot how we can create a 2D PCA projection, as we were facing some problems. 
Also, we asked how we can include a legend box that would contain the killers and their corresponding colors.
Unfortunately, we are unable to find the corresponding prompt and response. \\
In Q4 we asked Copilot how we can make a 2D PCA projection that overlays the one created in Q3.\\

\textbf{Prompt:}
Πως μπορούν να δημιουργηθούν PCA και πως μπορώ να τα κάνω overlay?

\textbf{Response:}
\begin{figure}[h]
    \centering
    \begin{minipage}[b]{0.49\textwidth}
        \centering
        \includegraphics[height=5cm, keepaspectratio]{./images/LLM answers/1.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.49\textwidth}
        \centering
        \includegraphics[height=5cm, keepaspectratio]{./images/LLM answers/2.png}
    \end{minipage}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{minipage}[b]{0.49\textwidth}
        \centering
        \includegraphics[height=5cm, keepaspectratio]{./images/LLM answers/3.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.49\textwidth}
        \centering
        \includegraphics[height=5cm, keepaspectratio]{./images/LLM answers/4.png}
    \end{minipage}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{minipage}[b]{0.49\textwidth}
        \centering
        \includegraphics[height=5cm, keepaspectratio]{./images/LLM answers/5.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.49\textwidth}
        \centering
        \includegraphics[height=5cm, keepaspectratio]{./images/LLM answers/6.png}
    \end{minipage}
\end{figure}

\newpage

\begin{figure}[h]
    \centering
    \begin{minipage}[b]{0.49\textwidth}
        \centering
        \includegraphics[height=5cm, keepaspectratio]{./images/LLM answers/7.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.49\textwidth}
        \centering
        \includegraphics[height=5cm, keepaspectratio]{./images/LLM answers/8.png}
    \end{minipage}
    \\[1em]
\end{figure}

\textbf{Prompt:}
Tί είναι το rbf kernel και πως μπορώ να το χρησιμοποιήσω σε python?

\textbf{Response:}
\begin{figure}[h]
    \centering
    \begin{minipage}[b]{0.49\textwidth}
        \centering
        \includegraphics[height=5cm, keepaspectratio]{./images/LLM answers/9.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.49\textwidth}
        \centering
        \includegraphics[height=5cm, keepaspectratio]{./images/LLM answers/10.png}
    \end{minipage}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{minipage}[b]{0.49\textwidth}
        \centering
        \includegraphics[height=5cm, keepaspectratio]{./images/LLM answers/11.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.49\textwidth}
        \centering
        \includegraphics[height=5cm, keepaspectratio]{./images/LLM answers/12.png}
    \end{minipage}
\end{figure}

\newpage
\begin{figure}[h]
    \centering
    \begin{minipage}[b]{0.51\textwidth}
        \includegraphics[width=\textwidth]{./images/LLM answers/13.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.47\textwidth}
        \includegraphics[width=\textwidth]{./images/LLM answers/14.png}
    \end{minipage}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{./images/LLM answers/15.png}
    \\[1em]
\end{figure}

\textbf{Prompt:}
Στο q6, τι είναι το baseline accuracy και πως κάνουμε shuffle?

\textbf{Response:}
\begin{figure}[h]
    \centering
    \begin{minipage}[b]{0.49\textwidth}
        \centering
        \includegraphics[height=5cm, keepaspectratio]{./images/LLM answers/16.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.49\textwidth}
        \centering
        \includegraphics[height=5cm, keepaspectratio]{./images/LLM answers/17.png}
    \end{minipage}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{minipage}[b]{0.49\textwidth}
        \centering
        \includegraphics[height=5cm, keepaspectratio]{./images/LLM answers/18.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.49\textwidth}
        \centering
        \includegraphics[height=5cm, keepaspectratio]{./images/LLM answers/19.png}
    \end{minipage}
\end{figure}

\newpage

\begin{figure}[h]
    \centering
    \begin{minipage}[b]{0.49\textwidth}
        \centering
        \includegraphics[height=5cm, keepaspectratio]{./images/LLM answers/20.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.49\textwidth}
        \centering
        \includegraphics[height=5cm, keepaspectratio]{./images/LLM answers/21.png}
    \end{minipage}
    \\[1em]
\end{figure}

\textbf{Prompt:}
Aυτός ο κώδικας απαντά στο ερώτημα? 

\begin{minted}[
    linenos,                
    breaklines=true,        
    breakanywhere=true,     
    breakautoindent=true,   
    bgcolor= gray!10,       
    fontsize=\small
]{python}
acc = accuracy_score(y_val, pred_val) 
print(f"VAL Accuracy: {acc}") 

# ======================== 
# Permutation Feature Importance 
# ======================== 

# Baseline 
accuracy A_base = acc 

# Get feature names after one-hot encoding 
ohe = model.named_steps["pre"].named_transformers_["onehot"] 
ohe_names = ohe.get_feature_names_out(CAT) 
all_feat = CONT + list(ohe_names) 

# Transform VAL once 
Z_val = model.named_steps["pre"].transform(X_val) 

importances = [] rng = np.random.default_rng(0) 

for j in range(Z_val.shape[1]): 
  Zp = Z_val.copy() 
  Zp[:, j] = rng.permutation(Zp[:, j]) # permute one feature 
  pred = model.named_steps["mlp"].predict(Zp) 
  Aj = accuracy_score(y_val, pred) 
  importances.append((all_feat[j], A_base - Aj)) 

# Sort by importance 
importances.sort(key=lambda x: x[1], reverse=True) 
top5 = importances[:5] 

print("\nTop 5 Most Important Features:") 
for name, imp in top5: 
  print(f"{name:35s} ΔA = {imp}") 

# ======================== 
# Plot top 5 
# ======================== 

names = [x[0] for x in top5] 
values = [x[1] for x in top5] 

plt.figure(figsize=(7,4)) 
plt.barh(names, values, color="skyblue")
plt.gca().invert_yaxis() 
plt.title("Top 5 Most Important Features (MLP)") 
plt.xlabel("Drop in Accuracy (ΔA)") 
plt.tight_layout() 
plt.savefig("q6_top5_features.png", dpi=150)
\end{minted} 

\textbf{Response:}
\begin{figure}[h]
    \centering
    \begin{minipage}[b]{0.49\textwidth}
        \centering
        \includegraphics[height=5cm, keepaspectratio]{./images/LLM answers/22.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.49\textwidth}
        \centering
        \includegraphics[height=5cm, keepaspectratio]{./images/LLM answers/23.png}
    \end{minipage}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{minipage}[b]{0.49\textwidth}
        \centering
        \includegraphics[height=5cm, keepaspectratio]{./images/LLM answers/24.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.49\textwidth}
        \centering
        \includegraphics[height=5cm, keepaspectratio]{./images/LLM answers/25.png}
    \end{minipage}
\end{figure}

\newpage

\begin{figure}[h]
    \centering
    \begin{minipage}[b]{0.5\textwidth}
        \includegraphics[width=\textwidth]{./images/LLM answers/26.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{./images/LLM answers/27.png}
    \end{minipage}
\end{figure}

\end{document}
